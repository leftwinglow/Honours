{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project outline\n",
    "### Could it have been predicted?\n",
    "\n",
    "In this project we will develop a series of safety pharmacology models using python based cheminformatic tools such as Rdkit, Scikitlearn, and pytorch. We want to see if machine learning cheminformatic toxicity models can single out the molecules that were ultimately removed from the market. I have collected a database of drugs that were withdrawn from the market due to toxicity for various reasons. This can be combined with a database of currently available medications to form a test set which can be used to test a model designed to predict toxicity. Many of the drugs that have been withdrawn over the years were withdrawn due to hepatotoxicity or DILI (drug induced liver injury) so I have collected a dataset of molecules with BSEP binding values to develop a model capable of predicting hepatoxicity. This will be the first model. If time allows, we will also gather data on other toxicities responsible for drug withdrawal such as binding the HERG (IKr) associated protein and potentially other secondary pharmacology assay targets such as Gprotein-coupledreceptors (GPCRs), enzymes, kinases, nuclear hormone receptors, ion channels and transporters.\n",
    "\n",
    "### References\n",
    "Assay Targets:\n",
    "\n",
    "Jenkinson, S., et al., A practical guide to secondary pharmacology in drug discovery. Journal of Pharmacological and Toxicological Methods, 2020. 105.\n",
    "\n",
    "BSEP Database:\n",
    "\n",
    "AbdulHameed, M.D.M., R. Liu, and A. Wallqvist, Using a Graph Convolutional Neural Network Model to Identify Bile Salt Export Pump Inhibitors. ACS Omega, 2023. 8(24): p. 21853-21861.\n",
    "\n",
    "Dataset of Withdrawn drugs:\n",
    "\n",
    "Siramshetty, V.B., et al., WITHDRAWN--a resource for withdrawn and discontinued drugs. Nucleic Acids Res, 2016. 44(D1): p. D1080-6.\n",
    "\n",
    "Onakpoya, I. J., Heneghan, C. J., & Aronson, J. K. (2016). Post-marketing withdrawal of 462 medicinal products because of adverse drug reactions: a systematic review of the world literature. BMC Medicine, 14, 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies\n",
    "\n",
    "### General\n",
    "* Pandas\n",
    "* Numpy\n",
    "* Seaborn\n",
    "\n",
    "### Datasets\n",
    "* TDC Tox\n",
    "\n",
    "### RDKit Modules\n",
    "* AllChem\n",
    "* rdMolDescriptors\n",
    "* IPythonConsole\n",
    "* Draw\n",
    "* DataStructs\n",
    "* Butina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "#---------------------- Therapeutic Drug Commons (TDC data) from https://tdcommons.ai/single_pred_tasks/tox/#dili-drug-induced-liver-injury\n",
    "from tdc.single_pred import Tox\n",
    "#---------------------- RDKit packages\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit import DataStructs\n",
    "from rdkit.ML.Cluster import Butina\n",
    "#-\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import ShuffleSplit, cross_validate,train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "### Reading, converting to pandas\n",
    "\n",
    "Read TDC Tox DILI Dataset & convert to Pandas dataframe.\\\n",
    "Rename columns to be more human-readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# This is Xu dataset\n",
    "tox_data = Tox(name = 'DILI')\n",
    "xu_df = tox_data.get_data()\n",
    "\n",
    "xu_df = (xu_df\n",
    "        .drop('Drug_ID', axis=1)\n",
    "        .rename(columns={'Drug' : 'SMILES', 'Y' : \"DILI?\"})\n",
    "        .astype({'DILI?' : 'Int16'})\n",
    ")\n",
    "\n",
    "xu_df.to_csv('Transformed_Data/Xu_DILI.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pubchempy as pcp\n",
    "\n",
    "onakpoya_df = pd.read_csv('Transformed_Data/Onakpoya_Drugs.csv', skiprows=[0]) # Read the table as csv\n",
    "\n",
    "onakpoya_df = (onakpoya_df\n",
    "    .filter(['Medicinal product', 'Reason for withdrawal']) # Drop irrelevant columns\n",
    "    .replace({'â€¡':''}, regex=True) # Remove uninterpretable characters\n",
    ")\n",
    "\n",
    "onakpoya_df = onakpoya_df[onakpoya_df['Reason for withdrawal'].str.endswith('Liver', na = False)] # Drop non-DILI related withdrawal\n",
    "onakpoya_df['Medicinal product'] = onakpoya_df['Medicinal product'].str.partition(' ')[0] # Only keep first word of drug name\n",
    "\n",
    "onakpoya_df['pcp_result'] = onakpoya_df['Medicinal product'].map(lambda x: pcp.get_compounds(identifier=x, namespace='name')) # Get pubchem CID for each compound\n",
    "onakpoya_df = onakpoya_df[onakpoya_df['pcp_result'].map(lambda d: len(d)) == 1] # Drop columns with multiple chemical identifiers\n",
    "\n",
    "onakpoya_df[\"pcp_result\"] = onakpoya_df[\"pcp_result\"].str[0] # Convert list of pubchempy compounds to str\n",
    "onakpoya_df[\"pcp_result\"] = onakpoya_df[\"pcp_result\"].apply(lambda x: x.isomeric_smiles) # Get isomeric smiles for pubchempy compounds\n",
    "\n",
    "onakpoya_df.columns = [\"Drug\", \"DILI?\", \"SMILES\"]\n",
    "\n",
    "onakpoya_df = (onakpoya_df\n",
    "    .filter([\"DILI?\", \"SMILES\"]) # Drop drug name\n",
    "    .reindex(columns = [\"SMILES\", \"DILI?\"]) # Reorder columns\n",
    "    .replace({'Liver' : 1}) # Liver = 1\n",
    ")\n",
    "\n",
    "onakpoya_df.to_csv('Transformed_Data/Onakpoya_DILI.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate the data from different sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tox_df = pd.concat([xu_df, onakpoya_df])\n",
    "\n",
    "tox_df.to_csv('Transformed_Data/Final_DILI.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 925 entries, 0 to 924\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   SMILES         925 non-null    object\n",
      " 1   BSEP ACTIVITY  925 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 14.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "Hameed_df = pd.read_excel(\"Transformed_Data/Hameed_BSEP.xlsx\", sheet_name=1, usecols = range(1, 3))\n",
    "\n",
    "print(Hameed_df.info(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Fingerprints\n",
    "\n",
    "Define function 'generate_fingerprints'\n",
    "Initialise empty list of Morgan fingerprints\n",
    "for molecules in a given dataframe, generate their morgan fingerprints and append them to the dataframe\n",
    "Reutrn appended dataframe as numpy array to analyse using 'shape'\n",
    "\n",
    "Run generate_fingerprints on each molecule in the dataframe\n",
    "\n",
    "Use shape to confirm success - First number should equal dataframe length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                SMILES  DILI?  \\\n",
      "0                                 CC(=O)OCC[N+](C)(C)C      0   \n",
      "1                                C[N+](C)(C)CC(=O)[O-]      0   \n",
      "2         O=C(NC(CO)C(O)c1ccc([N+](=O)[O-])cc1)C(Cl)Cl      0   \n",
      "3                                      O=C(O)c1ccccc1O      0   \n",
      "4                       CC(NC(C)(C)C)C(=O)c1cccc(Cl)c1      0   \n",
      "..                                                 ...    ...   \n",
      "437  CC1=CC=C(C=C1)C(=O)C2=CC(=C(C(=C2)O)O)[N+](=O)...      1   \n",
      "438    CN(CC(=O)O)C(=S)C1=CC=CC2=C1C=CC(=C2C(F)(F)F)OC      1   \n",
      "446  CC1=C(C2=C(CCC(O2)(C)COC3=CC=C(C=C3)CC4C(=O)NC...      1   \n",
      "447  C1[C@@H]2[C@@H](C2N)CN1C3=C(C=C4C(=O)C(=CN(C4=...      1   \n",
      "456  CCOC(C(=O)C1=CC=C(C=C1)C2=CC=CC=C2)NC3=CC=C(C=...      1   \n",
      "\n",
      "                                                  M3FP  \n",
      "0    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
      "1    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "2    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "4    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "..                                                 ...  \n",
      "437  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  \n",
      "438  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "446  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "447  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "456  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "\n",
      "[507 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from rdkit.Chem import MolFromSmiles\n",
    "\n",
    "def smiles_to_morgan_fingerprint(smiles):\n",
    "    mol = MolFromSmiles(smiles)\n",
    "    fingerprint = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits = 2048)\n",
    "    return fingerprint\n",
    "\n",
    "tox_df[\"M3FP\"] = [smiles_to_morgan_fingerprint(smiles) for smiles in tox_df[\"SMILES\"]]\n",
    "\n",
    "print(tox_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  1  2  3  4  5  6  7  8  9  ...  2039  2040  2041  2042  2043  2044  \\\n",
      "0  0  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0   \n",
      "1  0  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0   \n",
      "\n",
      "   2045  2046  2047  DILI?  \n",
      "0     0     0     0      0  \n",
      "1     0     0     0      0  \n",
      "\n",
      "[2 rows x 2049 columns]\n"
     ]
    }
   ],
   "source": [
    "morgan_df = tox_df[\"m3fp\"].apply(pd.Series)\n",
    "\n",
    "morgan_df.insert(len(morgan_df.columns), \"DILI?\", tox_df[\"DILI?\"].astype(int)) # Insert \"DILI?\" column as the last column\n",
    "\n",
    "morgan_df.columns = morgan_df.columns.astype(str) # Set all column titles to string - Required for model\n",
    "\n",
    "print(morgan_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Random Forest Regressor\n",
    "model_rf = ensemble.RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Instantiate a dict of paramaters of RFR\n",
    "params_rf = {\n",
    "            'bootstrap': [True, False],\n",
    "            'max_depth': [range(1, 4096, 10), None],\n",
    "            'max_features': ['auto', 'sqrt', 'log2', 1],\n",
    "            'min_samples_leaf': range(1, 8, 1),\n",
    "            'min_samples_split': range(1, 8, 1),\n",
    "            'n_estimators': range(10, 800, 10)\n",
    "            }\n",
    "\n",
    "X = morgan_df.iloc[:, 0:len(morgan_df.columns)-1] # Features\n",
    "y = morgan_df[\"DILI?\"].values # Labels\n",
    "\n",
    "#model_rf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Set up scoring methods for hyperparameter tuning\n",
    "scoring = ['r2', 'neg_root_mean_squared_error']\n",
    "\n",
    "# Enable multithreading functionality\n",
    "import multiprocessing\n",
    "n_jobs = multiprocessing.cpu_count()-1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Parameters: {'n_estimators': 710, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': None, 'bootstrap': False}\n",
      "Best score is 0.028006990579347368\n"
     ]
    }
   ],
   "source": [
    "# Run RandomizedSearchCV to optimise random forest hyperparameters\n",
    "rf_cv = RandomizedSearchCV(model_rf, params_rf, cv = 5, n_iter=12, n_jobs=n_jobs, random_state=42, scoring='r2') #5-fold precedended in AbdulHameed\n",
    "\n",
    "rf_cv.fit(X_test, y_test)\n",
    "\n",
    "# Print scores\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(rf_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(rf_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "A pipeline has not yet been optimized. Please call fit() first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Luke\\Documents\\University\\5th Year\\Honours Python\\get_fingerprints_tdc.ipynb Cell 19\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Luke/Documents/University/5th%20Year/Honours%20Python/get_fingerprints_tdc.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtpot\u001b[39;00m \u001b[39mimport\u001b[39;00m TPOTRegressor\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Luke/Documents/University/5th%20Year/Honours%20Python/get_fingerprints_tdc.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m pipeline_optimiser \u001b[39m=\u001b[39m TPOTRegressor(random_state \u001b[39m=\u001b[39m \u001b[39m42\u001b[39m, n_jobs\u001b[39m=\u001b[39mn_jobs)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Luke/Documents/University/5th%20Year/Honours%20Python/get_fingerprints_tdc.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m pipeline_optimiser\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Luke/Documents/University/5th%20Year/Honours%20Python/get_fingerprints_tdc.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(pipeline_optimiser\u001b[39m.\u001b[39mscore(X_test, y_test))\n",
      "File \u001b[1;32mc:\\Users\\Luke\\anaconda3\\envs\\honours\\Lib\\site-packages\\tpot\\base.py:864\u001b[0m, in \u001b[0;36mTPOTBase.fit\u001b[1;34m(self, features, target, sample_weight, groups)\u001b[0m\n\u001b[0;32m    861\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mSystemExit\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    862\u001b[0m         \u001b[39m# raise the exception if it's our last attempt\u001b[39;00m\n\u001b[0;32m    863\u001b[0m         \u001b[39mif\u001b[39;00m attempt \u001b[39m==\u001b[39m (attempts \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m--> 864\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    865\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Luke\\anaconda3\\envs\\honours\\Lib\\site-packages\\tpot\\base.py:855\u001b[0m, in \u001b[0;36mTPOTBase.fit\u001b[1;34m(self, features, target, sample_weight, groups)\u001b[0m\n\u001b[0;32m    852\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pbar, \u001b[39mtype\u001b[39m(\u001b[39mNone\u001b[39;00m)):\n\u001b[0;32m    853\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pbar\u001b[39m.\u001b[39mclose()\n\u001b[1;32m--> 855\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_top_pipeline()\n\u001b[0;32m    856\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_summary_of_best_pipeline(features, target)\n\u001b[0;32m    857\u001b[0m \u001b[39m# Delete the temporary cache before exiting\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Luke\\anaconda3\\envs\\honours\\Lib\\site-packages\\tpot\\base.py:962\u001b[0m, in \u001b[0;36mTPOTBase._update_top_pipeline\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    958\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_optimized_pareto_front_n_gens \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    959\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    960\u001b[0m     \u001b[39m# If user passes CTRL+C in initial generation, self._pareto_front (halloffame) shoule be not updated yet.\u001b[39;00m\n\u001b[0;32m    961\u001b[0m     \u001b[39m# need raise RuntimeError because no pipeline has been optimized\u001b[39;00m\n\u001b[1;32m--> 962\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    963\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mA pipeline has not yet been optimized. Please call fit() first.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    964\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: A pipeline has not yet been optimized. Please call fit() first."
     ]
    }
   ],
   "source": [
    "from tpot import TPOTRegressor\n",
    "\n",
    "pipeline_optimiser = TPOTRegressor(random_state = 42, n_jobs=n_jobs)\n",
    "\n",
    "pipeline_optimiser.fit(X_train, y_train)\n",
    "print(pipeline_optimiser.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Parameters: {'n_estimators': 120, 'max_depth': 5, 'learning_rate': 0.05}\n",
      "Best score is 0.3560953880782183\n"
     ]
    }
   ],
   "source": [
    "# Instantiate an XGBoost\n",
    "\n",
    "model_gb = ensemble.GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "params_gb = {\n",
    "            'n_estimators': range(0, 150, 10),\n",
    "            'max_depth': [2, 3, 5, 10, 15],\n",
    "            'learning_rate': [0.05, 0.1, 0.15, 0.20]\n",
    "            }\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "gb_cv = RandomizedSearchCV(model_gb, params_gb, cv = 5, n_iter=32, n_jobs=n_jobs, random_state=42, scoring='r2')\n",
    "\n",
    "gb_cv.fit(x, y)\n",
    "\n",
    "# Print scores\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(gb_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(gb_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thanks To\n",
    "\n",
    "https://www.youtube.com/watch?v=-oHqQBUyrQ0\n",
    "\n",
    "https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "\n",
    "https://github.com/PatWalters/practical_cheminformatics_posts/blob/main/solubility/literature_solubility_model.ipynb\n",
    "\n",
    "https://leftwinglow.github.io/BachelorsProject/\n",
    "\n",
    "https://github.com/gashawmg/Molecular-fingerprints/blob/main/Calculating%20molecular%20fingerprints%20available%20in%20RDkit%20.ipynb\n",
    "\n",
    "https://github.com/gashawmg/Avalon-fingerprints-for-machine-learning/blob/main/Avalon%20fingerprints%20for%20predictive%20modeling.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
