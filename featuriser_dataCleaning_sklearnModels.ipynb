{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project outline\n",
    "### Could it have been predicted?\n",
    "\n",
    "In this project we will develop a series of safety pharmacology models using python based cheminformatic tools such as Rdkit, Scikitlearn, and pytorch. We want to see if machine learning cheminformatic toxicity models can single out the molecules that were ultimately removed from the market. I have collected a database of drugs that were withdrawn from the market due to toxicity for various reasons. This can be combined with a database of currently available medications to form a test set which can be used to test a model designed to predict toxicity. Many of the drugs that have been withdrawn over the years were withdrawn due to hepatotoxicity or DILI (drug induced liver injury) so I have collected a dataset of molecules with BSEP binding values to develop a model capable of predicting hepatoxicity. This will be the first model. If time allows, we will also gather data on other toxicities responsible for drug withdrawal such as binding the HERG (IKr) associated protein and potentially other secondary pharmacology assay targets such as Gprotein-coupledreceptors (GPCRs), enzymes, kinases, nuclear hormone receptors, ion channels and transporters.\n",
    "\n",
    "### References\n",
    "Assay Targets:\n",
    "\n",
    "Jenkinson, S., et al., A practical guide to secondary pharmacology in drug discovery. Journal of Pharmacological and Toxicological Methods, 2020. 105.\n",
    "\n",
    "BSEP Database:\n",
    "\n",
    "AbdulHameed, M.D.M., R. Liu, and A. Wallqvist, Using a Graph Convolutional Neural Network Model to Identify Bile Salt Export Pump Inhibitors. ACS Omega, 2023. 8(24): p. 21853-21861.\n",
    "\n",
    "Dataset of Withdrawn drugs:\n",
    "\n",
    "Siramshetty, V.B., et al., WITHDRAWN--a resource for withdrawn and discontinued drugs. Nucleic Acids Res, 2016. 44(D1): p. D1080-6.\n",
    "\n",
    "Onakpoya, I. J., Heneghan, C. J., & Aronson, J. K. (2016). Post-marketing withdrawal of 462 medicinal products because of adverse drug reactions: a systematic review of the world literature. BMC Medicine, 14, 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pubchempy as pcp\n",
    "from yellowbrick import classifier\n",
    "from tqdm import tqdm\n",
    "#---------------------- Therapeutic Drug Commons (TDC data) from https://tdcommons.ai/single_pred_tasks/tox/#dili-drug-induced-liver-injury\n",
    "from tdc.single_pred import Tox\n",
    "#---------------------- RDKit packages\n",
    "from rdkit.Chem import AllChem\n",
    "from molfeat import trans\n",
    "#---------------------- scikit-learn packages\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "### DILI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to convert trade & generic drug names to SMILES strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_drug_name_with_smiles(dataframe: pd.DataFrame, drug_name_col: str) -> pd.DataFrame:\n",
    "    dataframe[drug_name_col] = dataframe[drug_name_col].map(lambda x: pcp.get_compounds(identifier=x, namespace='name')) # Get pubchem CID for each compound\n",
    "    dataframe = dataframe[dataframe[drug_name_col].map(lambda d: len(d)) == 1] # Drop columns with multiple chemical identifiers\n",
    "    dataframe[drug_name_col] = dataframe[drug_name_col].str[0] # Convert list of pubchempy compounds to str\n",
    "    dataframe[drug_name_col] = dataframe[drug_name_col].apply(lambda x: x.isomeric_smiles) # Get isomeric smiles for pubchempy compounds\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Xu DF from TDC library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "100%|██████████| 26.7k/26.7k [00:00<00:00, 13.3MiB/s]\n",
      "Loading...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "tox_data = Tox(name = 'DILI')\n",
    "xu_df = tox_data.get_data()\n",
    "\n",
    "xu_df = (xu_df\n",
    "        .drop('Drug_ID', axis=1)\n",
    "        .rename(columns={'Drug' : 'SMILES', 'Y' : 'DILI?'})\n",
    "        .astype({'DILI?' : 'Int16'})\n",
    ")\n",
    "\n",
    "xu_df.to_csv('Transformed_Data/Xu_DILI.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the Onakpoya DF from the withdrawn drugs list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onakpoya_df = pd.read_csv('Intermediate_Data/Onakpoya_Drugs.csv', skiprows = [0]) # Read the table as csv\n",
    "\n",
    "onakpoya_df = (onakpoya_df\n",
    "    .filter(['Medicinal product', 'Reason for withdrawal']) # Drop irrelevant columns\n",
    "    .replace({'‡':''}, regex=True) # Remove uninterpretable characters\n",
    ")\n",
    "\n",
    "onakpoya_df = onakpoya_df[onakpoya_df['Reason for withdrawal'].str.endswith('Liver', na = False)] # Drop non-DILI related withdrawal\n",
    "onakpoya_df['Medicinal product'] = onakpoya_df['Medicinal product'].str.partition(' ')[0] # Only keep first word of drug name\n",
    "\n",
    "onakpoya_df = replace_drug_name_with_smiles(onakpoya_df, \"Medicinal product\")\n",
    "\n",
    "onakpoya_df.columns = ['SMILES', 'DILI?']\n",
    "\n",
    "onakpoya_df = (onakpoya_df\n",
    "    .filter(['DILI?', 'SMILES']) # Drop drug name\n",
    "    .reindex(columns = ['SMILES', 'DILI?']) # Reorder columns\n",
    "    .replace({'Liver' : 1}) # Liver = 1\n",
    ")\n",
    "\n",
    "onakpoya_df.to_csv('Transformed_Data/Onakpoya_DILI2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the Livertox database, consider \"A\" DILI-positive, \"E\" DILI-negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   drug  dili\n",
      "4            CCCS(=O)(=O)NC1CC(C1)N(C)C2=NC=NC3=C2C=CN3     0\n",
      "6                                  CC(=O)NCCCS(=O)(=O)O     0\n",
      "12                              CC(=O)N[C@@H](CS)C(=O)O     0\n",
      "14    C1C[N+]2(CCC1[C@H](C2)OC(=O)C(C3=CC=CS3)(C4=CC...     0\n",
      "15    CC1=CC=C(C=C1)/C(=C\\CN2CCCC2)/C3=CC=CC(=N3)/C=...     0\n",
      "...                                                 ...   ...\n",
      "1493  COC(=O)[C@H]1[C@H](CC[C@@H]2[C@@H]1C[C@H]3C4=C...     0\n",
      "1496     CCN(C1=CC=CC(=C1)C2=CC=NC3=C(C=NN23)C#N)C(=O)C     0\n",
      "1497  CC(=O)N[C@@H]1[C@H](C=C(O[C@H]1[C@@H]([C@@H](C...     0\n",
      "1501                                               [Zn]     0\n",
      "1505    CC1=CC=C(C=C1)C2=C(N3C=C(C=CC3=N2)C)CC(=O)N(C)C     0\n",
      "\n",
      "[482 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "livertox_df = pd.read_excel('Raw_Data/LiverTox_DILI.xlsx', skiprows=range(2))\n",
    "\n",
    "values = ['A', 'E']\n",
    "\n",
    "livertox_df = (livertox_df\n",
    "    .query('`Likelihood Score` == @values')\n",
    "    .filter(['Ingredient', 'Likelihood Score'])\n",
    "    .rename(columns = {'Ingredient': 'drug', 'Likelihood Score': 'dili'})\n",
    "    .replace({'A': 1, 'E': 0})\n",
    ")\n",
    "\n",
    "livertox_df = replace_drug_name_with_smiles(livertox_df, \"drug\")\n",
    "\n",
    "print(livertox_df)\n",
    "\n",
    "livertox_df.to_csv('Transformed_Data/Livertox_DILI.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate the data from different sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tox_df = pd.concat([xu_df, onakpoya_df])\n",
    "\n",
    "tox_df.to_csv('Transformed_Data/Final_DILI.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BSEP inhibition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 925 entries, 0 to 924\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   SMILES         925 non-null    object\n",
      " 1   BSEP ACTIVITY  925 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 14.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "Hameed_df = pd.read_excel('Transformed_Data/Hameed_BSEP.xlsx', sheet_name=1, usecols = range(1, 3))\n",
    "\n",
    "print(Hameed_df.info(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Fingerprints\n",
    "\n",
    "Define function 'generate_fingerprints'\n",
    "Initialise empty list of Morgan fingerprints\n",
    "for molecules in a given dataframe, generate their morgan fingerprints and append them to the dataframe\n",
    "Reutrn appended dataframe as numpy array to analyse using 'shape'\n",
    "\n",
    "Run generate_fingerprints on each molecule in the dataframe\n",
    "\n",
    "Use shape to confirm success - First number should equal dataframe length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-28T12:35:36.631171400Z",
     "start_time": "2023-10-28T12:35:36.619278900Z"
    }
   },
   "outputs": [],
   "source": [
    "tox_df = pd.read_csv('Transformed_Data/Final_DILI.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 507 entries, 0 to 456\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   SMILES  507 non-null    object\n",
      " 1   DILI?   507 non-null    int64 \n",
      " 2   ecfp    507 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 15.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def generate_fp_column(dataframe, dataframe_smiles_col, fp_type: str) -> pd.DataFrame:\n",
    "    fp_transformer = trans.MoleculeTransformer(featurizer=f'{fp_type}')\n",
    "    dataframe[f\"{fp_type}\"] = fp_transformer.transform(dataframe_smiles_col.values)\n",
    "    return dataframe\n",
    "\n",
    "tox_df = generate_fp_column(tox_df, tox_df.SMILES, 'ecfp')\n",
    "\n",
    "print(tox_df.info(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 507 entries, 0 to 456\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   ecfp    507 non-null    object\n",
      " 1   DILI?   507 non-null    int32 \n",
      "dtypes: int32(1), object(1)\n",
      "memory usage: 9.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "morgan_df = pd.DataFrame(tox_df.iloc[:, 2])\n",
    "\n",
    "morgan_df.insert(len(morgan_df.columns), 'DILI?', tox_df['DILI?'].astype(int)) # Insert 'DILI?' column as the last column\n",
    "\n",
    "morgan_df.columns = morgan_df.columns.astype(str) # Set all column titles to string - Required for model\n",
    "\n",
    "print(morgan_df.info(2))\n",
    "\n",
    "#X = np.array(morgan_df['M3FP'])\n",
    "X = morgan_df.iloc[:, 0] # Features\n",
    "y = morgan_df.iloc[:, 1] # Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up scoring methods for hyperparameter tuning\n",
    "scoring = ['r2', 'neg_root_mean_squared_error']\n",
    "\n",
    "# Enable multithreading functionality\n",
    "import multiprocessing\n",
    "n_jobs = multiprocessing.cpu_count()-1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Random Forest Regressor\n",
    "model_rf = ensemble.RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Instantiate a dict of paramaters of RFR\n",
    "params_rf = {\n",
    "            'bootstrap': [True, False],\n",
    "            'max_depth': [range(1, 4096, 1000), None],\n",
    "            'max_features': ['auto', 'sqrt', 'log2', 1],\n",
    "            'min_samples_leaf': range(1, 8, 1),\n",
    "            'min_samples_split': range(1, 8, 1),\n",
    "            'n_estimators': range(10, 800, 10)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Parameters: {'n_estimators': 680, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 1, 'max_depth': None, 'bootstrap': True}\n",
      "Best score is 0.19498257839721264\n"
     ]
    }
   ],
   "source": [
    "# Run RandomizedSearchCV to optimise random forest hyperparameters\n",
    "rf_cv = RandomizedSearchCV(model_rf, params_rf, n_iter=64, n_jobs=n_jobs, random_state=42, scoring='r2') #5-fold precedended in AbdulHameed\n",
    "\n",
    "rf_cv.fit(list(X_train),y_train)\n",
    "\n",
    "# Print scores\n",
    "print('Tuned Logistic Regression Parameters: {}'.format(rf_cv.best_params_)) \n",
    "print('Best score is {}'.format(rf_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifier.confusion_matrix(rf_cv, list(X_test), y_test, cmap=\"Greens\")\n",
    "\n",
    "classifier.roc_auc(rf_cv, list(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "model_svm = svm.LinearSVC(random_state=42)\n",
    "\n",
    "model_svm.fit(list(X_train), y_train)\n",
    "\n",
    "classifier.confusion_matrix(model_svm, list(X_test), y_test, cmap=\"Greens\")\n",
    "classifier.roc_auc(model_svm, list(X_test), y_test, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTRegressor\n",
    "\n",
    "pipeline_optimiser = TPOTRegressor(random_state = 42, n_jobs=n_jobs)\n",
    "\n",
    "pipeline_optimiser.fit(X_train, y_train)\n",
    "print(pipeline_optimiser.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an XGBoost\n",
    "\n",
    "model_gb = ensemble.GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "params_gb = {\n",
    "            'max_depth': [2, 3, 5, 10, 15],\n",
    "            'learning_rate': [0.05, 0.1, 0.15, 0.20]\n",
    "            }\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "gb_cv = RandomizedSearchCV(model_gb, params_gb, cv = 5, n_iter=32, n_jobs=n_jobs, random_state=42, scoring='r2')\n",
    "\n",
    "gb_cv.fit(list(X_train), y_train)\n",
    "\n",
    "# Print scores\n",
    "print('Tuned Logistic Regression Parameters: {}'.format(gb_cv.best_params_)) \n",
    "print('Best score is {}'.format(gb_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.confusion_matrix(gb_cv, list(X_test), y_test, cmap=\"Greens\")\n",
    "classifier.roc_auc(gb_cv, list(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thanks To\n",
    "\n",
    "https://www.youtube.com/watch?v=-oHqQBUyrQ0\n",
    "\n",
    "https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "\n",
    "https://github.com/PatWalters/practical_cheminformatics_posts/blob/main/solubility/literature_solubility_model.ipynb\n",
    "\n",
    "https://leftwinglow.github.io/BachelorsProject/\n",
    "\n",
    "https://github.com/gashawmg/Molecular-fingerprints/blob/main/Calculating%20molecular%20fingerprints%20available%20in%20RDkit%20.ipynb\n",
    "\n",
    "https://github.com/gashawmg/Avalon-fingerprints-for-machine-learning/blob/main/Avalon%20fingerprints%20for%20predictive%20modeling.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
